{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (60000, 28, 28)\n",
      "y_train.shape: (60000,)\n",
      "x_test.shape: (10000, 28, 28)\n",
      "y_test.shape: (10000,)\n",
      "[5 0 4 1 9]\n"
     ]
    }
   ],
   "source": [
    "print('x_train.shape:', x_train.shape)\n",
    "print('y_train.shape:', y_train.shape)\n",
    "print('x_test.shape:', x_test.shape)\n",
    "print('y_test.shape:', y_test.shape)\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.3372549  0.11372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.3372549  0.3372549  0.55294118\n",
      " 1.         1.         1.         1.         0.3372549  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.44705882 0.77647059\n",
      " 1.         1.         1.         1.         1.         0.66666667\n",
      " 0.66666667 1.         0.88627451 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.11372549\n",
      " 0.11372549 0.11372549 0.77647059 0.55294118 0.11372549 0.3372549\n",
      " 0.66666667 1.         1.         1.         0.88627451 0.66666667\n",
      " 0.55294118 0.3372549  0.         0.         0.         1.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.88627451 1.         1.\n",
      " 1.         1.         1.         1.         1.         1.\n",
      " 0.77647059 0.3372549  0.         0.         0.         0.\n",
      " 0.         0.         0.77647059 1.         0.3372549  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.         1.         1.         1.         1.\n",
      " 1.         0.66666667 0.55294118 0.11372549 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.66666667\n",
      " 1.         1.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.3372549  0.3372549  0.3372549  0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.66666667 1.         1.         0.3372549\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.55294118\n",
      " 1.         1.         0.22352941 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.44705882 1.         1.         0.55294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.3372549\n",
      " 1.         1.         0.55294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3372549  1.         1.         0.55294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.22352941\n",
      " 1.         1.         0.66666667 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.         1.         0.55294118\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.3372549  1.         0.77647059 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.88627451 0.88627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         0.88627451 0.11372549 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.55294118 0.66666667 1.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train/255.\n",
    "x_test = x_test/255.\n",
    "\n",
    "x_train, x_valid = train_test_split(x_train, test_size=0.2)\n",
    "print(x_valid[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy  as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_dim = 32\n",
    "\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "encoder = Model(input_img, encoded)\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "decorder = Model(encoded_input, decorder_layer(encoded_input))\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss = 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/80\n",
      "48000/48000 [==============================] - 20s 418us/step - loss: 0.1757 - val_loss: 0.1745\n",
      "Epoch 2/80\n",
      "48000/48000 [==============================] - 20s 406us/step - loss: 0.1733 - val_loss: 0.1722\n",
      "Epoch 3/80\n",
      "48000/48000 [==============================] - 20s 409us/step - loss: 0.1711 - val_loss: 0.1700\n",
      "Epoch 4/80\n",
      "48000/48000 [==============================] - 24s 497us/step - loss: 0.1689 - val_loss: 0.1678\n",
      "Epoch 5/80\n",
      "48000/48000 [==============================] - 28s 583us/step - loss: 0.1668 - val_loss: 0.1658\n",
      "Epoch 6/80\n",
      "48000/48000 [==============================] - 27s 567us/step - loss: 0.1649 - val_loss: 0.1639\n",
      "Epoch 7/80\n",
      "48000/48000 [==============================] - 23s 472us/step - loss: 0.1630 - val_loss: 0.1620\n",
      "Epoch 8/80\n",
      "48000/48000 [==============================] - 22s 449us/step - loss: 0.1611 - val_loss: 0.1603\n",
      "Epoch 9/80\n",
      "48000/48000 [==============================] - 26s 531us/step - loss: 0.1594 - val_loss: 0.1585\n",
      "Epoch 10/80\n",
      "48000/48000 [==============================] - 22s 466us/step - loss: 0.1577 - val_loss: 0.1569\n",
      "Epoch 11/80\n",
      "48000/48000 [==============================] - 21s 445us/step - loss: 0.1562 - val_loss: 0.1554\n",
      "Epoch 12/80\n",
      "48000/48000 [==============================] - 21s 444us/step - loss: 0.1546 - val_loss: 0.1539\n",
      "Epoch 13/80\n",
      "48000/48000 [==============================] - 23s 471us/step - loss: 0.1531 - val_loss: 0.1524\n",
      "Epoch 14/80\n",
      "48000/48000 [==============================] - 25s 516us/step - loss: 0.1517 - val_loss: 0.1510\n",
      "Epoch 15/80\n",
      "48000/48000 [==============================] - 25s 530us/step - loss: 0.1503 - val_loss: 0.1497\n",
      "Epoch 16/80\n",
      "48000/48000 [==============================] - 23s 485us/step - loss: 0.1490 - val_loss: 0.1484\n",
      "Epoch 17/80\n",
      "48000/48000 [==============================] - 21s 438us/step - loss: 0.1477 - val_loss: 0.1471\n",
      "Epoch 18/80\n",
      "48000/48000 [==============================] - 21s 427us/step - loss: 0.1465 - val_loss: 0.1459\n",
      "Epoch 19/80\n",
      "48000/48000 [==============================] - 23s 477us/step - loss: 0.1453 - val_loss: 0.1447\n",
      "Epoch 20/80\n",
      "48000/48000 [==============================] - 25s 521us/step - loss: 0.1441 - val_loss: 0.1435\n",
      "Epoch 21/80\n",
      "48000/48000 [==============================] - 29s 605us/step - loss: 0.1429 - val_loss: 0.1424\n",
      "Epoch 22/80\n",
      "48000/48000 [==============================] - 30s 619us/step - loss: 0.1418 - val_loss: 0.1413\n",
      "Epoch 23/80\n",
      "48000/48000 [==============================] - 30s 622us/step - loss: 0.1407 - val_loss: 0.1402\n",
      "Epoch 24/80\n",
      "48000/48000 [==============================] - 28s 589us/step - loss: 0.1397 - val_loss: 0.1392\n",
      "Epoch 25/80\n",
      "48000/48000 [==============================] - 30s 632us/step - loss: 0.1386 - val_loss: 0.1382\n",
      "Epoch 26/80\n",
      "48000/48000 [==============================] - 25s 524us/step - loss: 0.1376 - val_loss: 0.1372\n",
      "Epoch 27/80\n",
      "48000/48000 [==============================] - 29s 611us/step - loss: 0.1366 - val_loss: 0.1362\n",
      "Epoch 28/80\n",
      "48000/48000 [==============================] - 30s 633us/step - loss: 0.1357 - val_loss: 0.1352\n",
      "Epoch 29/80\n",
      "48000/48000 [==============================] - 26s 536us/step - loss: 0.1347 - val_loss: 0.1343\n",
      "Epoch 30/80\n",
      "48000/48000 [==============================] - 20s 410us/step - loss: 0.1338 - val_loss: 0.1334\n",
      "Epoch 31/80\n",
      "48000/48000 [==============================] - 20s 419us/step - loss: 0.1329 - val_loss: 0.1325\n",
      "Epoch 32/80\n",
      "48000/48000 [==============================] - 20s 418us/step - loss: 0.1320 - val_loss: 0.1317\n",
      "Epoch 33/80\n",
      "48000/48000 [==============================] - 22s 465us/step - loss: 0.1312 - val_loss: 0.1308\n",
      "Epoch 34/80\n",
      "48000/48000 [==============================] - 21s 428us/step - loss: 0.1303 - val_loss: 0.1299\n",
      "Epoch 35/80\n",
      "48000/48000 [==============================] - 20s 421us/step - loss: 0.1295 - val_loss: 0.1291\n",
      "Epoch 36/80\n",
      "48000/48000 [==============================] - 20s 420us/step - loss: 0.1287 - val_loss: 0.1283\n",
      "Epoch 37/80\n",
      "48000/48000 [==============================] - 20s 417us/step - loss: 0.1279 - val_loss: 0.1275\n",
      "Epoch 38/80\n",
      "48000/48000 [==============================] - 20s 418us/step - loss: 0.1271 - val_loss: 0.1267\n",
      "Epoch 39/80\n",
      "48000/48000 [==============================] - 20s 424us/step - loss: 0.1263 - val_loss: 0.1260\n",
      "Epoch 40/80\n",
      "48000/48000 [==============================] - 20s 416us/step - loss: 0.1256 - val_loss: 0.1253\n",
      "Epoch 41/80\n",
      "48000/48000 [==============================] - 20s 416us/step - loss: 0.1248 - val_loss: 0.1245\n",
      "Epoch 42/80\n",
      "48000/48000 [==============================] - 20s 419us/step - loss: 0.1241 - val_loss: 0.1238\n",
      "Epoch 43/80\n",
      "48000/48000 [==============================] - 20s 414us/step - loss: 0.1234 - val_loss: 0.1231\n",
      "Epoch 44/80\n",
      "48000/48000 [==============================] - 20s 425us/step - loss: 0.1227 - val_loss: 0.1224\n",
      "Epoch 45/80\n",
      "48000/48000 [==============================] - 20s 421us/step - loss: 0.1220 - val_loss: 0.1218\n",
      "Epoch 46/80\n",
      "48000/48000 [==============================] - 19s 405us/step - loss: 0.1214 - val_loss: 0.1211\n",
      "Epoch 47/80\n",
      "48000/48000 [==============================] - 20s 416us/step - loss: 0.1207 - val_loss: 0.1205\n",
      "Epoch 48/80\n",
      "48000/48000 [==============================] - 20s 422us/step - loss: 0.1201 - val_loss: 0.1199\n",
      "Epoch 49/80\n",
      "48000/48000 [==============================] - 20s 415us/step - loss: 0.1195 - val_loss: 0.1193\n",
      "Epoch 50/80\n",
      "48000/48000 [==============================] - 20s 420us/step - loss: 0.1189 - val_loss: 0.1187\n",
      "Epoch 51/80\n",
      "48000/48000 [==============================] - 21s 427us/step - loss: 0.1183 - val_loss: 0.1181\n",
      "Epoch 52/80\n",
      "48000/48000 [==============================] - 21s 432us/step - loss: 0.1178 - val_loss: 0.1175\n",
      "Epoch 53/80\n",
      "48000/48000 [==============================] - 21s 444us/step - loss: 0.1172 - val_loss: 0.1170\n",
      "Epoch 54/80\n",
      "48000/48000 [==============================] - 23s 483us/step - loss: 0.1167 - val_loss: 0.1165\n",
      "Epoch 55/80\n",
      "48000/48000 [==============================] - 23s 469us/step - loss: 0.1162 - val_loss: 0.1160\n",
      "Epoch 56/80\n",
      "48000/48000 [==============================] - 23s 485us/step - loss: 0.1157 - val_loss: 0.1155\n",
      "Epoch 57/80\n",
      "48000/48000 [==============================] - 21s 445us/step - loss: 0.1152 - val_loss: 0.1150\n",
      "Epoch 58/80\n",
      "48000/48000 [==============================] - 21s 436us/step - loss: 0.1147 - val_loss: 0.1145\n",
      "Epoch 59/80\n",
      "48000/48000 [==============================] - 21s 428us/step - loss: 0.1142 - val_loss: 0.1141\n",
      "Epoch 60/80\n",
      "48000/48000 [==============================] - 21s 443us/step - loss: 0.1138 - val_loss: 0.1136\n",
      "Epoch 61/80\n",
      "48000/48000 [==============================] - 24s 494us/step - loss: 0.1133 - val_loss: 0.1132\n",
      "Epoch 62/80\n",
      "48000/48000 [==============================] - 22s 449us/step - loss: 0.1129 - val_loss: 0.1128\n",
      "Epoch 63/80\n",
      "48000/48000 [==============================] - 24s 499us/step - loss: 0.1125 - val_loss: 0.1124\n",
      "Epoch 64/80\n",
      "48000/48000 [==============================] - 25s 525us/step - loss: 0.1121 - val_loss: 0.1120\n",
      "Epoch 65/80\n",
      "48000/48000 [==============================] - 21s 441us/step - loss: 0.1117 - val_loss: 0.1116\n",
      "Epoch 66/80\n",
      "48000/48000 [==============================] - 28s 578us/step - loss: 0.1113 - val_loss: 0.1112\n",
      "Epoch 67/80\n",
      "48000/48000 [==============================] - 29s 599us/step - loss: 0.1110 - val_loss: 0.1108\n",
      "Epoch 68/80\n",
      "48000/48000 [==============================] - 31s 651us/step - loss: 0.1106 - val_loss: 0.1105\n",
      "Epoch 69/80\n",
      "48000/48000 [==============================] - 30s 620us/step - loss: 0.1103 - val_loss: 0.1101\n",
      "Epoch 70/80\n",
      "48000/48000 [==============================] - 25s 525us/step - loss: 0.1099 - val_loss: 0.1098\n",
      "Epoch 71/80\n",
      "48000/48000 [==============================] - 30s 623us/step - loss: 0.1096 - val_loss: 0.1095\n",
      "Epoch 72/80\n",
      "48000/48000 [==============================] - 31s 639us/step - loss: 0.1093 - val_loss: 0.1092\n",
      "Epoch 73/80\n",
      "48000/48000 [==============================] - 31s 636us/step - loss: 0.1090 - val_loss: 0.1089\n",
      "Epoch 74/80\n",
      "48000/48000 [==============================] - 31s 638us/step - loss: 0.1087 - val_loss: 0.1086\n",
      "Epoch 75/80\n",
      "48000/48000 [==============================] - 27s 560us/step - loss: 0.1084 - val_loss: 0.1083\n",
      "Epoch 76/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 22s 459us/step - loss: 0.1081 - val_loss: 0.1080\n",
      "Epoch 77/80\n",
      "48000/48000 [==============================] - 22s 463us/step - loss: 0.1078 - val_loss: 0.1077\n",
      "Epoch 78/80\n",
      "48000/48000 [==============================] - 26s 545us/step - loss: 0.1075 - val_loss: 0.1074\n",
      "Epoch 79/80\n",
      "48000/48000 [==============================] - 29s 606us/step - loss: 0.1073 - val_loss: 0.1072\n",
      "Epoch 80/80\n",
      "48000/48000 [==============================] - 31s 643us/step - loss: 0.1070 - val_loss: 0.1069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e243e79e8>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder.fit(x_train, x_train\n",
    "               ,epochs=80\n",
    "               ,batch_size=512\n",
    "               ,shuffle=True\n",
    "               ,validation_data=(x_valid, x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "[0.20562759 0.8671434  0.3735369  0.4692888  0.07265055 0.11943388\n",
      " 0.6888802  0.64203894 0.12317175 0.18462878 0.02670139 0.30387598\n",
      " 0.01100329 0.24183238 0.7855693  0.98894155 0.32531142 0.21415645\n",
      " 0.26228243 0.9940151  0.83415073 0.9124749  0.5261309  0.6120485\n",
      " 0.9706416  0.81845725 0.35149544 0.20021403 0.31457642 0.11531833\n",
      " 0.16560414 0.26803178 0.88325715 0.34006834 0.27806425 0.90224046\n",
      " 0.43774563 0.22010058 0.48301727 0.3403365  0.04486611 0.90479225\n",
      " 0.5583691  0.25164473 0.4687781  0.9424231  0.3872497  0.9810841\n",
      " 0.71369874 0.9931884  0.8081694  0.83353204 0.44611177 0.21499747\n",
      " 0.5034926  0.32084352 0.1978451  0.40353394 0.23433033 0.5546681\n",
      " 0.8561677  0.5446955  0.5261141  0.18667993 0.7227603  0.3627199\n",
      " 0.8934306  0.6430873  0.7151192  0.8858316  0.10067534 0.9654026\n",
      " 0.94603205 0.93530107 0.8859205  0.7602112  0.5700417  0.95084023\n",
      " 0.3012507  0.0528985  0.3300317  0.04154271 0.09695554 0.927034\n",
      " 0.20661631 0.08920267 0.7196605  0.39817712 0.08610299 0.7916095\n",
      " 0.98722976 0.21394295 0.7357283  0.10212016 0.6103771  0.04296532\n",
      " 0.0324468  0.9375005  0.95822144 0.9218811  0.27077222 0.1299507\n",
      " 0.29108632 0.41357297 0.12546748 0.31514364 0.8554949  0.42098027\n",
      " 0.46903086 0.7705388  0.38621446 0.8475931  0.72123945 0.27076006\n",
      " 0.26624405 0.08714777 0.18020558 0.92860585 0.95159775 0.8471217\n",
      " 0.6620788  0.7492101  0.26255375 0.01128545 0.00230616 0.55348104\n",
      " 0.16650957 0.73644996 0.7670678  0.76100856 0.32702744 0.11260054\n",
      " 0.73109645 0.9896872  0.7217614  0.89368945 0.2139076  0.42540175\n",
      " 0.23935115 0.07269323 0.57303053 0.8491316  0.16900462 0.13377303\n",
      " 0.35959762 0.12880152 0.5336112  0.82663745 0.92953146 0.17636454\n",
      " 0.4265873  0.0445677  0.01721478 0.08281562 0.8790138  0.08682835\n",
      " 0.19638386 0.9707961  0.12864256 0.6708057  0.7117     0.28250703\n",
      " 0.5798508  0.00745487 0.07397357 0.07994077 0.6581233  0.18698195\n",
      " 0.15192902 0.7583616  0.5023265  0.71517086 0.35606623 0.06863734\n",
      " 0.49406895 0.16242924 0.45694616 0.6018625  0.9879172  0.50748223\n",
      " 0.36138013 0.98435366 0.93770933 0.37163565 0.4760561  0.88821733\n",
      " 0.74428123 0.91335034 0.7307446  0.0795767  0.98238033 0.10583419\n",
      " 0.6166954  0.48187876 0.98512566 0.7666608  0.3057824  0.0326311\n",
      " 0.68416095 0.8252539  0.7147951  0.6461353  0.44851273 0.46481463\n",
      " 0.21357068 0.10424402 0.5311744  0.15920722 0.22704765 0.39228627\n",
      " 0.7504108  0.83907855 0.8752726  0.19511995 0.7545295  0.16383424\n",
      " 0.05897626 0.34583142 0.02273405 0.3748768  0.41648346 0.02068171\n",
      " 0.18120632 0.1799492  0.5420236  0.7954946  0.51329434 0.8709281\n",
      " 0.9451034  0.1214377  0.65565896 0.5591068  0.70619667 0.09701887\n",
      " 0.00886154 0.29384243 0.31830928 0.04253453 0.3532039  0.9047195\n",
      " 0.13850841 0.73201215 0.9706954  0.5946365  0.27913615 0.58028674\n",
      " 0.67570984 0.80878854 0.60700274 0.76204646 0.41120118 0.84489596\n",
      " 0.11472493 0.2164095  0.8909807  0.91152275 0.07671022 0.6629102\n",
      " 0.87329483 0.8753352  0.8748888  0.0485625  0.02320996 0.17056191\n",
      " 0.16972402 0.9909384  0.23550892 0.35180476 0.568151   0.7272308\n",
      " 0.0842714  0.11901957 0.17362124 0.16307935 0.18580526 0.42697135\n",
      " 0.7026545  0.62250596 0.13288972 0.17854783 0.10242751 0.9271487\n",
      " 0.40058514 0.1149528  0.73189193 0.25743794 0.12090492 0.04557422\n",
      " 0.07115981 0.94311094 0.64726233 0.8419368  0.2168274  0.11865428\n",
      " 0.8971925  0.53655463 0.62971604 0.5430058  0.7677863  0.05730969\n",
      " 0.05947748 0.14917102 0.5297835  0.53484845 0.32218128 0.18482503\n",
      " 0.6184212  0.7159928  0.12860325 0.01435792 0.60942376 0.67206836\n",
      " 0.60672987 0.6771345  0.9614117  0.45670095 0.3657951  0.25681108\n",
      " 0.03030825 0.2053301  0.09038574 0.37595072 0.03677562 0.14298746\n",
      " 0.265212   0.11491713 0.97450614 0.4382565  0.54491526 0.14561832\n",
      " 0.8255523  0.02501562 0.9310685  0.5844488  0.6487471  0.0420059\n",
      " 0.7237066  0.6437212  0.9278239  0.86263263 0.6749753  0.16800329\n",
      " 0.5218782  0.51971394 0.7500285  0.7532822  0.323461   0.33043084\n",
      " 0.36875343 0.7912319  0.35877144 0.1835087  0.06371719 0.9320836\n",
      " 0.05117306 0.61803454 0.8386469  0.87329066 0.07055345 0.09819981\n",
      " 0.808512   0.5245993  0.4553997  0.5385987  0.38755003 0.09873492\n",
      " 0.93801284 0.395539   0.28448206 0.04413414 0.29978186 0.32832175\n",
      " 0.6314646  0.9910222  0.76299465 0.28663266 0.20144024 0.6341814\n",
      " 0.9188589  0.10686263 0.08982301 0.9553473  0.9264442  0.31552243\n",
      " 0.6414722  0.9765919  0.44908744 0.05846649 0.8542832  0.5452539\n",
      " 0.6892037  0.7327738  0.68081975 0.30960765 0.9768467  0.95304334\n",
      " 0.7403726  0.64285755 0.1328283  0.5123964  0.18318132 0.05958885\n",
      " 0.10699773 0.97025853 0.17301944 0.68264914 0.52323174 0.6540747\n",
      " 0.80215317 0.9052651  0.7663476  0.9578291  0.1717518  0.78789675\n",
      " 0.10244206 0.2533248  0.85986316 0.4979185  0.2500623  0.78981185\n",
      " 0.18116358 0.1371238  0.01077279 0.150399   0.25739062 0.27247092\n",
      " 0.6484567  0.04721639 0.7214543  0.8449472  0.83354604 0.66302514\n",
      " 0.40533513 0.52833116 0.7964055  0.6279795  0.12476087 0.01977444\n",
      " 0.64268786 0.6664121  0.12930754 0.06987911 0.8329992  0.18945691\n",
      " 0.36674327 0.8260833  0.999637   0.9608786  0.01234618 0.06176245\n",
      " 0.0124267  0.18765354 0.28215665 0.19096106 0.8840357  0.61830354\n",
      " 0.5597536  0.05766651 0.28130805 0.9827554  0.6772143  0.96396816\n",
      " 0.04866198 0.9104011  0.4666473  0.9514098  0.4485861  0.48458886\n",
      " 0.9156773  0.57834643 0.7627497  0.09131148 0.33232504 0.44212827\n",
      " 0.6192235  0.87954754 0.7237724  0.6322404  0.265141   0.9888185\n",
      " 0.8591111  0.89887583 0.8713192  0.7100348  0.6266413  0.88141924\n",
      " 0.8316875  0.18724757 0.11833423 0.4129613  0.26373157 0.35678315\n",
      " 0.97631574 0.4530207  0.63441277 0.7508661  0.4119278  0.8528812\n",
      " 0.47559577 0.5501533  0.12936872 0.1403782  0.5270807  0.64973384\n",
      " 0.05139112 0.971603   0.95643437 0.19498757 0.12235358 0.3195449\n",
      " 0.79434276 0.57474524 0.69198513 0.08464423 0.34023955 0.4307853\n",
      " 0.2395907  0.3337297  0.5916345  0.8233921  0.25593555 0.8432462\n",
      " 0.6801221  0.8408177  0.02977249 0.97333956 0.93704814 0.25489998\n",
      " 0.11639404 0.02316996 0.597464   0.56384265 0.93428576 0.5369293\n",
      " 0.72005606 0.65195704 0.48280686 0.8160958  0.22275984 0.2192528\n",
      " 0.850224   0.80398095 0.04554859 0.6151554  0.39828336 0.42774084\n",
      " 0.2115142  0.6987889  0.5165761  0.9617947  0.7811576  0.48295534\n",
      " 0.8893961  0.2550043  0.2471757  0.13037929 0.9051728  0.3412084\n",
      " 0.9934592  0.8391771  0.08381394 0.55126786 0.63629454 0.03331366\n",
      " 0.67409015 0.5011805  0.61455417 0.11860085 0.36821932 0.02340958\n",
      " 0.9459815  0.9011291  0.7630042  0.99463356 0.62421167 0.43292138\n",
      " 0.7648876  0.6856431  0.3487544  0.7213575  0.05646077 0.01898167\n",
      " 0.36244777 0.9095001  0.43008798 0.919695   0.42701828 0.38051236\n",
      " 0.8983388  0.01832896 0.287987   0.49384412 0.07079586 0.91607463\n",
      " 0.8676198  0.16888303 0.52477545 0.8970758  0.6416108  0.9237586\n",
      " 0.6682931  0.89365566 0.44607207 0.83797526 0.9486595  0.9622402\n",
      " 0.63368374 0.6210671  0.34358743 0.3854048  0.88615334 0.72170764\n",
      " 0.46908498 0.58488536 0.08058366 0.56130666 0.54508597 0.45656404\n",
      " 0.24796084 0.237062   0.74305534 0.26771757 0.42198533 0.96560156\n",
      " 0.46067    0.6251574  0.9866723  0.4662919  0.3992415  0.82806444\n",
      " 0.8828874  0.15406829 0.34901363 0.4717097  0.06043378 0.16344279\n",
      " 0.5426467  0.41087353 0.6015232  0.02305213 0.9751914  0.54331964\n",
      " 0.34644046 0.1235545  0.8796879  0.6861272  0.3381288  0.3356049\n",
      " 0.88499135 0.9053612  0.8958223  0.78021026 0.77018714 0.63830996\n",
      " 0.29247403 0.87702537 0.6374131  0.4173872  0.1292114  0.21763033\n",
      " 0.28603354 0.65588254 0.38550648 0.07228461 0.678643   0.05147216\n",
      " 0.8494288  0.03404394 0.39194041 0.4154388  0.48405877 0.4193522\n",
      " 0.27756453 0.10075358 0.06888473 0.568616   0.04749703 0.27831393\n",
      " 0.22536096 0.43748558 0.19916168 0.842144   0.03850403 0.00490028\n",
      " 0.542884   0.7365307  0.41725385 0.4046996  0.01463467 0.40871766\n",
      " 0.03933755 0.6396818  0.13584548 0.14768687 0.7781533  0.04576585\n",
      " 0.4253211  0.36635765 0.61035234 0.06561434 0.3850935  0.8571968\n",
      " 0.32964993 0.52465004 0.6467735  0.32489434 0.944173   0.16725048\n",
      " 0.69512326 0.4444775  0.15753144 0.3878978  0.72791106 0.6280735\n",
      " 0.4100295  0.8012983  0.25752124 0.9308549  0.93054724 0.9448395\n",
      " 0.88681376 0.6838847  0.44724748 0.83265066 0.5740876  0.17425117\n",
      " 0.6014815  0.9461462  0.26816225 0.89021575 0.94731575 0.54262775\n",
      " 0.01453793 0.22726163 0.00627628 0.03796193 0.47734013 0.23725572\n",
      " 0.90514654 0.69007534 0.5930267  0.4879347  0.06272459 0.42176047\n",
      " 0.37573153 0.08976537 0.6701518  0.0457032  0.7969763  0.04538319\n",
      " 0.04037461 0.1916557  0.5837365  0.0396226  0.30822977 0.70386696\n",
      " 0.52930796 0.7736151  0.94717133 0.01486266 0.46665043 0.56929904\n",
      " 0.8027191  0.1152488  0.63193095 0.14578232 0.582271   0.5323474\n",
      " 0.5896069  0.43828917 0.12967223 0.23653075 0.62846243 0.7402811\n",
      " 0.8340632  0.8827724  0.09781986 0.21099654 0.56182575 0.9667647\n",
      " 0.37300026 0.47389808 0.2489619  0.45870566]\n",
      "0.48773548\n"
     ]
    }
   ],
   "source": [
    "encoded_img = encoder.predict(x_test)\n",
    "decoded_img = decorder.predict(encoded_img)\n",
    "print(decoded_img.shape)\n",
    "print(decoded_img[0])\n",
    "print(decoded_img.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 10000 samples\n",
      "Epoch 1/115\n",
      "48000/48000 [==============================] - 57s 1ms/step - loss: 2.3168 - acc: 0.1042 - val_loss: 2.3131 - val_acc: 0.0932\n",
      "Epoch 2/115\n",
      "48000/48000 [==============================] - 44s 916us/step - loss: 2.3023 - acc: 0.1136 - val_loss: 2.3112 - val_acc: 0.1085\n",
      "Epoch 3/115\n",
      "48000/48000 [==============================] - 54s 1ms/step - loss: 2.2957 - acc: 0.1222 - val_loss: 2.3024 - val_acc: 0.1522\n",
      "Epoch 4/115\n",
      "48000/48000 [==============================] - 54s 1ms/step - loss: 2.2879 - acc: 0.1308 - val_loss: 2.3069 - val_acc: 0.1149\n",
      "Epoch 5/115\n",
      "48000/48000 [==============================] - 50s 1ms/step - loss: 2.2769 - acc: 0.1368 - val_loss: 2.3072 - val_acc: 0.1077\n",
      "Epoch 6/115\n",
      "48000/48000 [==============================] - 40s 842us/step - loss: 2.2645 - acc: 0.1496 - val_loss: 2.2991 - val_acc: 0.1280\n",
      "Epoch 7/115\n",
      "48000/48000 [==============================] - 43s 890us/step - loss: 2.2512 - acc: 0.1595 - val_loss: 2.3268 - val_acc: 0.1160\n",
      "Epoch 8/115\n",
      "48000/48000 [==============================] - 40s 837us/step - loss: 2.2349 - acc: 0.1704 - val_loss: 2.3365 - val_acc: 0.1268\n",
      "Epoch 9/115\n",
      "48000/48000 [==============================] - 41s 848us/step - loss: 2.2176 - acc: 0.1788 - val_loss: 2.3427 - val_acc: 0.1216\n",
      "Epoch 10/115\n",
      "48000/48000 [==============================] - 40s 824us/step - loss: 2.2009 - acc: 0.1891 - val_loss: 2.3693 - val_acc: 0.1110\n",
      "Epoch 11/115\n",
      "48000/48000 [==============================] - 40s 831us/step - loss: 2.1810 - acc: 0.2003 - val_loss: 2.3495 - val_acc: 0.1298\n",
      "Epoch 12/115\n",
      "48000/48000 [==============================] - 40s 828us/step - loss: 2.1619 - acc: 0.2090 - val_loss: 2.3837 - val_acc: 0.1162\n",
      "Epoch 13/115\n",
      "48000/48000 [==============================] - 40s 830us/step - loss: 2.1425 - acc: 0.2175 - val_loss: 2.3981 - val_acc: 0.1104\n",
      "Epoch 14/115\n",
      "48000/48000 [==============================] - 40s 829us/step - loss: 2.1204 - acc: 0.2287 - val_loss: 2.3877 - val_acc: 0.1170\n",
      "Epoch 15/115\n",
      "48000/48000 [==============================] - 41s 849us/step - loss: 2.1037 - acc: 0.2365 - val_loss: 2.3807 - val_acc: 0.1312\n",
      "Epoch 16/115\n",
      "48000/48000 [==============================] - 40s 823us/step - loss: 2.0817 - acc: 0.2473 - val_loss: 2.4101 - val_acc: 0.1135\n",
      "Epoch 17/115\n",
      "48000/48000 [==============================] - 41s 857us/step - loss: 2.0589 - acc: 0.2586 - val_loss: 2.4398 - val_acc: 0.1157\n",
      "Epoch 18/115\n",
      "48000/48000 [==============================] - 40s 826us/step - loss: 2.0412 - acc: 0.2625 - val_loss: 2.4305 - val_acc: 0.1150\n",
      "Epoch 19/115\n",
      "48000/48000 [==============================] - 40s 825us/step - loss: 2.0197 - acc: 0.2756 - val_loss: 2.4628 - val_acc: 0.1197\n",
      "Epoch 20/115\n",
      "48000/48000 [==============================] - 40s 836us/step - loss: 2.0062 - acc: 0.2792 - val_loss: 2.4579 - val_acc: 0.1287\n",
      "Epoch 21/115\n",
      "48000/48000 [==============================] - 39s 822us/step - loss: 1.9821 - acc: 0.2895 - val_loss: 2.5175 - val_acc: 0.1016\n",
      "Epoch 22/115\n",
      "48000/48000 [==============================] - 39s 813us/step - loss: 1.9598 - acc: 0.2980 - val_loss: 2.5378 - val_acc: 0.0979\n",
      "Epoch 23/115\n",
      "48000/48000 [==============================] - 33s 692us/step - loss: 1.9454 - acc: 0.3044 - val_loss: 2.5082 - val_acc: 0.1284\n",
      "Epoch 24/115\n",
      "48000/48000 [==============================] - 33s 693us/step - loss: 1.9286 - acc: 0.3103 - val_loss: 2.5448 - val_acc: 0.1217\n",
      "Epoch 25/115\n",
      "48000/48000 [==============================] - 34s 698us/step - loss: 1.9117 - acc: 0.3174 - val_loss: 2.5621 - val_acc: 0.1096\n",
      "Epoch 26/115\n",
      "48000/48000 [==============================] - 34s 708us/step - loss: 1.8932 - acc: 0.3250 - val_loss: 2.6005 - val_acc: 0.1084\n",
      "Epoch 27/115\n",
      "48000/48000 [==============================] - 33s 695us/step - loss: 1.8744 - acc: 0.3338 - val_loss: 2.6281 - val_acc: 0.1126\n",
      "Epoch 28/115\n",
      "48000/48000 [==============================] - 33s 690us/step - loss: 1.8607 - acc: 0.3365 - val_loss: 2.5362 - val_acc: 0.1304\n",
      "Epoch 29/115\n",
      "48000/48000 [==============================] - 33s 695us/step - loss: 1.8435 - acc: 0.3444 - val_loss: 2.6094 - val_acc: 0.1229\n",
      "Epoch 30/115\n",
      "48000/48000 [==============================] - 34s 701us/step - loss: 1.8309 - acc: 0.3504 - val_loss: 2.6755 - val_acc: 0.1067\n",
      "Epoch 31/115\n",
      "48000/48000 [==============================] - 34s 703us/step - loss: 1.8087 - acc: 0.3564 - val_loss: 2.6506 - val_acc: 0.1079\n",
      "Epoch 32/115\n",
      "48000/48000 [==============================] - 36s 744us/step - loss: 1.7987 - acc: 0.3608 - val_loss: 2.7402 - val_acc: 0.1029\n",
      "Epoch 33/115\n",
      "48000/48000 [==============================] - 35s 729us/step - loss: 1.7819 - acc: 0.3691 - val_loss: 2.6826 - val_acc: 0.1188\n",
      "Epoch 34/115\n",
      "48000/48000 [==============================] - 34s 718us/step - loss: 1.7639 - acc: 0.3754 - val_loss: 2.7800 - val_acc: 0.1056\n",
      "Epoch 35/115\n",
      "48000/48000 [==============================] - 34s 703us/step - loss: 1.7542 - acc: 0.3776 - val_loss: 2.8054 - val_acc: 0.1003\n",
      "Epoch 36/115\n",
      "48000/48000 [==============================] - 34s 709us/step - loss: 1.7401 - acc: 0.3826 - val_loss: 2.7974 - val_acc: 0.1025\n",
      "Epoch 37/115\n",
      "48000/48000 [==============================] - 34s 711us/step - loss: 1.7268 - acc: 0.3879 - val_loss: 2.8065 - val_acc: 0.1054\n",
      "Epoch 38/115\n",
      "48000/48000 [==============================] - 34s 706us/step - loss: 1.7129 - acc: 0.3959 - val_loss: 2.7750 - val_acc: 0.1152\n",
      "Epoch 39/115\n",
      "48000/48000 [==============================] - 34s 703us/step - loss: 1.7008 - acc: 0.3955 - val_loss: 2.8679 - val_acc: 0.0995\n",
      "Epoch 40/115\n",
      "48000/48000 [==============================] - 34s 716us/step - loss: 1.6848 - acc: 0.4051 - val_loss: 2.8303 - val_acc: 0.1055\n",
      "Epoch 41/115\n",
      "48000/48000 [==============================] - 34s 712us/step - loss: 1.6722 - acc: 0.4071 - val_loss: 2.8587 - val_acc: 0.1086\n",
      "Epoch 42/115\n",
      "48000/48000 [==============================] - 34s 713us/step - loss: 1.6578 - acc: 0.4137 - val_loss: 2.9119 - val_acc: 0.1088\n",
      "Epoch 43/115\n",
      "48000/48000 [==============================] - 34s 709us/step - loss: 1.6498 - acc: 0.4170 - val_loss: 2.9244 - val_acc: 0.1085\n",
      "Epoch 44/115\n",
      "48000/48000 [==============================] - 35s 722us/step - loss: 1.6396 - acc: 0.4211 - val_loss: 2.8659 - val_acc: 0.1125\n",
      "Epoch 45/115\n",
      "48000/48000 [==============================] - 34s 715us/step - loss: 1.6288 - acc: 0.4243 - val_loss: 2.9014 - val_acc: 0.1045\n",
      "Epoch 46/115\n",
      "48000/48000 [==============================] - 34s 708us/step - loss: 1.6172 - acc: 0.4308 - val_loss: 2.9724 - val_acc: 0.1036\n",
      "Epoch 47/115\n",
      "48000/48000 [==============================] - 36s 743us/step - loss: 1.6078 - acc: 0.4343 - val_loss: 3.0622 - val_acc: 0.0980\n",
      "Epoch 48/115\n",
      "48000/48000 [==============================] - 35s 734us/step - loss: 1.5977 - acc: 0.4331 - val_loss: 3.0430 - val_acc: 0.0995\n",
      "Epoch 49/115\n",
      "48000/48000 [==============================] - 35s 720us/step - loss: 1.5897 - acc: 0.4402 - val_loss: 3.0331 - val_acc: 0.1042\n",
      "Epoch 50/115\n",
      "48000/48000 [==============================] - 40s 827us/step - loss: 1.5778 - acc: 0.4441 - val_loss: 3.0524 - val_acc: 0.1059\n",
      "Epoch 51/115\n",
      "48000/48000 [==============================] - 41s 856us/step - loss: 1.5658 - acc: 0.4492 - val_loss: 3.0261 - val_acc: 0.1074\n",
      "Epoch 52/115\n",
      "48000/48000 [==============================] - 39s 815us/step - loss: 1.5582 - acc: 0.4517 - val_loss: 3.0250 - val_acc: 0.1087\n",
      "Epoch 53/115\n",
      "48000/48000 [==============================] - 38s 798us/step - loss: 1.5477 - acc: 0.4549 - val_loss: 3.0990 - val_acc: 0.1065\n",
      "Epoch 54/115\n",
      "48000/48000 [==============================] - 38s 799us/step - loss: 1.5400 - acc: 0.4577 - val_loss: 3.1185 - val_acc: 0.1133\n",
      "Epoch 55/115\n",
      "48000/48000 [==============================] - 38s 800us/step - loss: 1.5269 - acc: 0.4628 - val_loss: 3.1340 - val_acc: 0.1121\n",
      "Epoch 56/115\n",
      "48000/48000 [==============================] - 39s 803us/step - loss: 1.5225 - acc: 0.4626 - val_loss: 3.1690 - val_acc: 0.0981\n",
      "Epoch 57/115\n",
      "48000/48000 [==============================] - 39s 808us/step - loss: 1.5110 - acc: 0.4688 - val_loss: 3.2070 - val_acc: 0.1046\n",
      "Epoch 58/115\n",
      "48000/48000 [==============================] - 39s 810us/step - loss: 1.5008 - acc: 0.4708 - val_loss: 3.1375 - val_acc: 0.1037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/115\n",
      "48000/48000 [==============================] - 32s 675us/step - loss: 1.4960 - acc: 0.4736 - val_loss: 3.1466 - val_acc: 0.1150\n",
      "Epoch 60/115\n",
      "48000/48000 [==============================] - 32s 670us/step - loss: 1.4896 - acc: 0.4765 - val_loss: 3.1965 - val_acc: 0.1033\n",
      "Epoch 61/115\n",
      "48000/48000 [==============================] - 32s 667us/step - loss: 1.4756 - acc: 0.4808 - val_loss: 3.2836 - val_acc: 0.0897\n",
      "Epoch 62/115\n",
      "48000/48000 [==============================] - 32s 660us/step - loss: 1.4709 - acc: 0.4849 - val_loss: 3.2199 - val_acc: 0.1094\n",
      "Epoch 63/115\n",
      "48000/48000 [==============================] - 32s 658us/step - loss: 1.4623 - acc: 0.4857 - val_loss: 3.1967 - val_acc: 0.1165\n",
      "Epoch 64/115\n",
      "48000/48000 [==============================] - 32s 666us/step - loss: 1.4520 - acc: 0.4885 - val_loss: 3.2684 - val_acc: 0.1106\n",
      "Epoch 65/115\n",
      "48000/48000 [==============================] - 32s 664us/step - loss: 1.4472 - acc: 0.4903 - val_loss: 3.2525 - val_acc: 0.1228\n",
      "Epoch 66/115\n",
      "48000/48000 [==============================] - 32s 660us/step - loss: 1.4349 - acc: 0.4950 - val_loss: 3.3128 - val_acc: 0.1025\n",
      "Epoch 67/115\n",
      "48000/48000 [==============================] - 32s 662us/step - loss: 1.4311 - acc: 0.4985 - val_loss: 3.3389 - val_acc: 0.1108\n",
      "Epoch 68/115\n",
      "48000/48000 [==============================] - 33s 683us/step - loss: 1.4250 - acc: 0.5001 - val_loss: 3.3530 - val_acc: 0.1018\n",
      "Epoch 69/115\n",
      "48000/48000 [==============================] - 34s 706us/step - loss: 1.4150 - acc: 0.5032 - val_loss: 3.4843 - val_acc: 0.0983\n",
      "Epoch 70/115\n",
      "48000/48000 [==============================] - 31s 653us/step - loss: 1.4062 - acc: 0.5062 - val_loss: 3.4296 - val_acc: 0.0974\n",
      "Epoch 71/115\n",
      "48000/48000 [==============================] - 31s 650us/step - loss: 1.3988 - acc: 0.5098 - val_loss: 3.4139 - val_acc: 0.1045\n",
      "Epoch 72/115\n",
      "48000/48000 [==============================] - 31s 652us/step - loss: 1.3981 - acc: 0.5081 - val_loss: 3.4119 - val_acc: 0.1090\n",
      "Epoch 73/115\n",
      "48000/48000 [==============================] - 31s 644us/step - loss: 1.3891 - acc: 0.5121 - val_loss: 3.4091 - val_acc: 0.1089\n",
      "Epoch 74/115\n",
      "48000/48000 [==============================] - 31s 651us/step - loss: 1.3802 - acc: 0.5142 - val_loss: 3.5009 - val_acc: 0.0997\n",
      "Epoch 75/115\n",
      "48000/48000 [==============================] - 32s 660us/step - loss: 1.3779 - acc: 0.5164 - val_loss: 3.3856 - val_acc: 0.1095\n",
      "Epoch 76/115\n",
      "48000/48000 [==============================] - 31s 650us/step - loss: 1.3726 - acc: 0.5193 - val_loss: 3.4418 - val_acc: 0.1160\n",
      "Epoch 77/115\n",
      "48000/48000 [==============================] - 32s 657us/step - loss: 1.3624 - acc: 0.5205 - val_loss: 3.5934 - val_acc: 0.0967\n",
      "Epoch 78/115\n",
      "48000/48000 [==============================] - 31s 651us/step - loss: 1.3512 - acc: 0.5249 - val_loss: 3.5203 - val_acc: 0.1161\n",
      "Epoch 79/115\n",
      "48000/48000 [==============================] - 32s 666us/step - loss: 1.3528 - acc: 0.5250 - val_loss: 3.5576 - val_acc: 0.1063\n",
      "Epoch 80/115\n",
      "48000/48000 [==============================] - 31s 649us/step - loss: 1.3359 - acc: 0.5267 - val_loss: 3.5635 - val_acc: 0.1138\n",
      "Epoch 81/115\n",
      "48000/48000 [==============================] - 31s 654us/step - loss: 1.3466 - acc: 0.5274 - val_loss: 3.5858 - val_acc: 0.1143\n",
      "Epoch 82/115\n",
      "48000/48000 [==============================] - 32s 660us/step - loss: 1.3315 - acc: 0.5325 - val_loss: 3.5903 - val_acc: 0.1150\n",
      "Epoch 83/115\n",
      "48000/48000 [==============================] - 32s 668us/step - loss: 1.3270 - acc: 0.5343 - val_loss: 3.6754 - val_acc: 0.1047\n",
      "Epoch 84/115\n",
      "48000/48000 [==============================] - 32s 664us/step - loss: 1.3244 - acc: 0.5339 - val_loss: 3.7134 - val_acc: 0.1053\n",
      "Epoch 85/115\n",
      "48000/48000 [==============================] - 32s 671us/step - loss: 1.3184 - acc: 0.5351 - val_loss: 3.6975 - val_acc: 0.1052\n",
      "Epoch 86/115\n",
      "48000/48000 [==============================] - 35s 720us/step - loss: 1.3120 - acc: 0.5398 - val_loss: 3.6290 - val_acc: 0.1060\n",
      "Epoch 87/115\n",
      "48000/48000 [==============================] - 33s 678us/step - loss: 1.3098 - acc: 0.5395 - val_loss: 3.7505 - val_acc: 0.1018\n",
      "Epoch 88/115\n",
      "48000/48000 [==============================] - 32s 662us/step - loss: 1.2969 - acc: 0.5461 - val_loss: 3.6478 - val_acc: 0.1090\n",
      "Epoch 89/115\n",
      "48000/48000 [==============================] - 32s 662us/step - loss: 1.2971 - acc: 0.5473 - val_loss: 3.7764 - val_acc: 0.1046\n",
      "Epoch 90/115\n",
      "48000/48000 [==============================] - 33s 679us/step - loss: 1.2901 - acc: 0.5460 - val_loss: 3.7346 - val_acc: 0.1040\n",
      "Epoch 91/115\n",
      "48000/48000 [==============================] - 32s 668us/step - loss: 1.2843 - acc: 0.5484 - val_loss: 3.7806 - val_acc: 0.1078\n",
      "Epoch 92/115\n",
      "48000/48000 [==============================] - 32s 668us/step - loss: 1.2822 - acc: 0.5493 - val_loss: 3.8391 - val_acc: 0.1041\n",
      "Epoch 93/115\n",
      "48000/48000 [==============================] - 32s 670us/step - loss: 1.2695 - acc: 0.5524 - val_loss: 3.8049 - val_acc: 0.1055\n",
      "Epoch 94/115\n",
      "48000/48000 [==============================] - 33s 683us/step - loss: 1.2768 - acc: 0.5530 - val_loss: 3.8480 - val_acc: 0.1047\n",
      "Epoch 95/115\n",
      "48000/48000 [==============================] - 32s 671us/step - loss: 1.2669 - acc: 0.5537 - val_loss: 3.8073 - val_acc: 0.1093\n",
      "Epoch 96/115\n",
      "48000/48000 [==============================] - 32s 673us/step - loss: 1.2554 - acc: 0.5599 - val_loss: 3.7946 - val_acc: 0.1133\n",
      "Epoch 97/115\n",
      "48000/48000 [==============================] - 32s 673us/step - loss: 1.2561 - acc: 0.5586 - val_loss: 3.9277 - val_acc: 0.1047\n",
      "Epoch 98/115\n",
      "48000/48000 [==============================] - 33s 679us/step - loss: 1.2549 - acc: 0.5595 - val_loss: 3.8764 - val_acc: 0.1065\n",
      "Epoch 99/115\n",
      "48000/48000 [==============================] - 32s 674us/step - loss: 1.2466 - acc: 0.5646 - val_loss: 3.9350 - val_acc: 0.1126\n",
      "Epoch 100/115\n",
      "48000/48000 [==============================] - 33s 682us/step - loss: 1.2388 - acc: 0.5651 - val_loss: 3.9646 - val_acc: 0.1100\n",
      "Epoch 101/115\n",
      "48000/48000 [==============================] - 33s 692us/step - loss: 1.2376 - acc: 0.5669 - val_loss: 3.8887 - val_acc: 0.1111\n",
      "Epoch 102/115\n",
      "48000/48000 [==============================] - 33s 687us/step - loss: 1.2307 - acc: 0.5678 - val_loss: 3.9054 - val_acc: 0.1129\n",
      "Epoch 103/115\n",
      "48000/48000 [==============================] - 33s 684us/step - loss: 1.2266 - acc: 0.5700 - val_loss: 4.0135 - val_acc: 0.1080\n",
      "Epoch 104/115\n",
      "48000/48000 [==============================] - 33s 678us/step - loss: 1.2214 - acc: 0.5729 - val_loss: 4.0339 - val_acc: 0.1024\n",
      "Epoch 105/115\n",
      "48000/48000 [==============================] - 33s 693us/step - loss: 1.2232 - acc: 0.5713 - val_loss: 4.0691 - val_acc: 0.1018\n",
      "Epoch 106/115\n",
      "48000/48000 [==============================] - 33s 677us/step - loss: 1.2134 - acc: 0.5751 - val_loss: 4.0701 - val_acc: 0.1042\n",
      "Epoch 107/115\n",
      "48000/48000 [==============================] - 32s 676us/step - loss: 1.2133 - acc: 0.5743 - val_loss: 4.0695 - val_acc: 0.1035\n",
      "Epoch 108/115\n",
      "48000/48000 [==============================] - 33s 686us/step - loss: 1.2057 - acc: 0.5780 - val_loss: 4.0828 - val_acc: 0.1029\n",
      "Epoch 109/115\n",
      "48000/48000 [==============================] - 33s 689us/step - loss: 1.2028 - acc: 0.5783 - val_loss: 3.9780 - val_acc: 0.1115\n",
      "Epoch 110/115\n",
      "48000/48000 [==============================] - 33s 681us/step - loss: 1.2029 - acc: 0.5776 - val_loss: 4.2300 - val_acc: 0.1010\n",
      "Epoch 111/115\n",
      "48000/48000 [==============================] - 33s 680us/step - loss: 1.1952 - acc: 0.5836 - val_loss: 4.1249 - val_acc: 0.1055\n",
      "Epoch 112/115\n",
      "48000/48000 [==============================] - 34s 700us/step - loss: 1.1924 - acc: 0.5821 - val_loss: 4.2207 - val_acc: 0.1028\n",
      "Epoch 113/115\n",
      "48000/48000 [==============================] - 33s 687us/step - loss: 1.1891 - acc: 0.5825 - val_loss: 4.1617 - val_acc: 0.1040\n",
      "Epoch 114/115\n",
      "48000/48000 [==============================] - 33s 689us/step - loss: 1.1799 - acc: 0.5876 - val_loss: 4.1568 - val_acc: 0.1067\n",
      "Epoch 115/115\n",
      "48000/48000 [==============================] - 33s 692us/step - loss: 1.1756 - acc: 0.5887 - val_loss: 4.1500 - val_acc: 0.1105\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.callbacks  import TensorBoard\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "y_train, y_valid = train_test_split(y_train, test_size=0.2)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=512, input_shape=(784,), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "model.compile(optimizer=RMSprop(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#tsb=TensorBoard(log_dir='./logs')\n",
    "history=model.fit(x_train,y_train\n",
    "                 ,batch_size=128\n",
    "                 ,epochs=115\n",
    "                 ,verbose=1\n",
    "                 ,validation_data=(x_test, y_test)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [0]*28000\n",
    "\n",
    "for y, item in enumerate(y_pred):\n",
    "    for x , name in enumerate(item):\n",
    "        if name == 1: \n",
    "            result[y] = x\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "imgid = np.array(np.arange(1,28001)).astype(int)\n",
    "result = pd.DataFrame(result, imgid, columns = [\"Label\"])\n",
    "result.index.name = \"ImageId\"\n",
    "result.to_csv(\"digits_result17.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
